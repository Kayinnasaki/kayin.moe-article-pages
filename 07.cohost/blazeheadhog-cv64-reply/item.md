---
title: "BlazeHedgehog's post in the CV64 Thread"
date: '23-06-2024'
publish_date: '23-06-2024'
taxonomy:
  category:
    - 'Unlisted'
  tag:
   - 'Chost-Repost'
---

!! The following post is not mine, but from **BlazeHedgehog**. It is being archived due to it's need in the Toyification article. What is left out is the reply with whom Blaze was arguing with, but the points Blaze make are just as important valid without the context. The original post, while it still exists, can be found [here](https://cohost.org/blazehedgehog/post/6583760-cherry-picking-is-on)

*... Several replies down in the CV64 review reply chain.*

It's a difference of scaling down versus blowing up, imo. CRTs could push past even HD resolutions as early as 1996 and by the mid 2000's were brushing up on 4K pixel counts. I remember friends bragging about playing Quake 3 at over 2K pixels wide and tall.

But even by 2001, PC games like Half-Life still had to be compressed to work on a PS2. Menus had to be redesigned. Fonts enlarged. It had to be readable at 480i, and the system struggled to maintain all the detail of that four year old PC game at a stable 30fps.

Similarly, yeah, sure, a lot of Saturn and Playstation and in some cases even Nintendo 64 games got PC ports, but what's the reputation, there? There have always been quick and dirty PC ports of console games that don't respect the source material. The one I always go back to is the PC port of Spider-man: Web of Shadows, which is missing half of the lighting effects and even on the most powerful PCs on earth is plagued by stuttering that seems to stem from the game trying to phone home to replace billboards with real world advertisements using a server that is no longer functional.

<p><iframe width="560" height="315" src="https://www.youtube.com/embed/Qbq00ySS5Bg" title="" frameBorder="0"   allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"  allowFullScreen><br>Powered by <a href="https://youtubeembedcode.com">youtube embed code</a> and <a href="https://skipboregler.com/da/">skip bo regler</a></iframe></p>

Furthermore, Disney currently sells Cars 2 on Steam (no joke, a legitimately great racing game)... except the PC port is the PS2 version of the game, so just like Web of Shadows, it is missing significant amounts of effects from the console version.

![](cars1.jpg?lightbox)
![](cars2.jpg?lightbox)

A screenshot of Cars 2 from the Steam Community page A screenshot of the same track in Cars 2 for the Xbox 360

My point is that games are often designed for a specific format and you can't wave "but PC ports!" around, because ports are ports, and they have problems that go both ways.

Some games do actually need big chonky pixels to hide behind.

![](mgs.webp?lightbox)

<hr>

*Blaze gets politely accused of Cherry Picking*

<hr>

Cherry picking is one thing, but my point was that games are developed for the limitations of their platform. Much past 1996, PC games are developed to be viewed at the highest resolution you can throw at them, whereas console games up to about 2005 were developed with a smeary CRT in mind. Even as far back as the 1980's, a console game development station was both a computer monitor and a TV to compare against:

![](jpdev.png?lightbox)

Even dating back to the Apple II, blurrier screens were a feature that was used to not only hide imperfections, but in some case create new effects. I forget if it actually was the Apple II, but there was an old computer platform that stretched the limits of its very small color palette and could display beyond its range by relying on how the blurry screen handled certain dithering patterns. And I know you know that, Shadow Hog.

These games were not made to be viewed cleanly. They depend on signal ghosting, and aperture grilles. Treating them like PC games where you can infinitely sharpen out jaggies and clean up dithering is like...

Let's take movie restoration. Let's talk about film grain. Did you know that a lot of modern film restoration is about completely removing the natural grain that comes from shooting with a film camera, and then re-adding some amount of film grain back in at the end of the restoration process? Films need to look a little dirty in order to look right.

Even modern, 100% digitally shot movies will add fake film grain.

This is why I ended my post with a picture of the PC version of Metal Gear Solid. Not only does adding in new texture filtering completely wreck how that game renders faces, but it's just not meant to be viewed so cleanly. Metal Gear Solid is not meant to look like this:

![A raw, 1:1 screenshot of Metal Gear Solid, grainy and dithered to hell. This is basically exactly like a PS1 renders the game, though I have artificially brightened it to make the grain more apparent in the darker corners.](mgs1.png?lightbox)

And it's not meant to look like this:

![Using Duckstation, I have bumped Metal Gear Solid up to run at 1440p. I have enabled true color rendering to remove the dithering, and I have forced texture filtering.](mgs1.png?lightbox)

It was made to look like this:

![Metal Gear Solid running through multiple filters; one accurately simulates NTSC signal ghosting and the other applies a CRT aperture grille.](mgs1.png?lightbox)

Now, look. I'll level with you. I've kind of gotten into the weeds here, this is maybe coming across more aggressive than it needs to, and there are definitely examples where you can actually upscale one of these games and it looks great (Parappa the Rapper, for instance, only looks better and better at higher resolutions). But not always.

Ultimately my point here is that you shouldn't treat console games like PC games and vice versa. Some of these things do actually rely on being chunky and fuzzy in order to look right.

Resident Evil 2 blown up at 4x its resolution, and then doubled again to make the mixed pixel resolution stand out.

![](re2.png?lightbox)